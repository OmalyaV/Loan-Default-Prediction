{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.13",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [
        {
          "sourceId": 7653020,
          "sourceType": "datasetVersion",
          "datasetId": 4461571
        }
      ],
      "dockerImageVersionId": 30646,
      "isInternetEnabled": false,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": false
    },
    "colab": {
      "name": "Loan Status Prediction",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/OmalyaV/Loan-Default-Prediction/blob/main/Loan_Status_Prediction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "source": [
        "\n",
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES\n",
        "# TO THE CORRECT LOCATION (/kaggle/input) IN YOUR NOTEBOOK,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "\n",
        "import os\n",
        "import sys\n",
        "from tempfile import NamedTemporaryFile\n",
        "from urllib.request import urlopen\n",
        "from urllib.parse import unquote, urlparse\n",
        "from urllib.error import HTTPError\n",
        "from zipfile import ZipFile\n",
        "import tarfile\n",
        "import shutil\n",
        "\n",
        "CHUNK_SIZE = 40960\n",
        "DATA_SOURCE_MAPPING = 'debt-default-prediction:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-data-sets%2F4461571%2F7653020%2Fbundle%2Farchive.zip%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20240302%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20240302T085015Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D6b9d49c81d159fcdc13287f19fd248cecb6a5d69cec85662507d2b4bce47e971ba54e0c5b5aaeea681a4f4897ab3ae6466405b3aee189c81d292ba1a8554d463981899ba7019eb628a405179e2be99ee3b9edcbc1cde067e8d9a2d2168e2e3b4a823007be95cc633ed6d5f43d47d4ba99daf546fed1ce6c6e8bdf0abdeacde87ed16b2f8d1e8dd8a765ce1f88275490c235a1819f6851c3ced31633bafbb0ca72f27ad06e599c226c1ad8aa5b192c4cfabf89f42c954e82dc9dd89d16299287509b22fbc98629dd68f037b094b384263bc460b92c4bab156c94cc935b31fd893f9485024e8ea906bd568e05eff4042926f3946b1eb92a22cb0eb0ed419ba3a81'\n",
        "\n",
        "KAGGLE_INPUT_PATH='/kaggle/input'\n",
        "KAGGLE_WORKING_PATH='/kaggle/working'\n",
        "KAGGLE_SYMLINK='kaggle'\n",
        "\n",
        "!umount /kaggle/input/ 2> /dev/null\n",
        "shutil.rmtree('/kaggle/input', ignore_errors=True)\n",
        "os.makedirs(KAGGLE_INPUT_PATH, 0o777, exist_ok=True)\n",
        "os.makedirs(KAGGLE_WORKING_PATH, 0o777, exist_ok=True)\n",
        "\n",
        "try:\n",
        "  os.symlink(KAGGLE_INPUT_PATH, os.path.join(\"..\", 'input'), target_is_directory=True)\n",
        "except FileExistsError:\n",
        "  pass\n",
        "try:\n",
        "  os.symlink(KAGGLE_WORKING_PATH, os.path.join(\"..\", 'working'), target_is_directory=True)\n",
        "except FileExistsError:\n",
        "  pass\n",
        "\n",
        "for data_source_mapping in DATA_SOURCE_MAPPING.split(','):\n",
        "    directory, download_url_encoded = data_source_mapping.split(':')\n",
        "    download_url = unquote(download_url_encoded)\n",
        "    filename = urlparse(download_url).path\n",
        "    destination_path = os.path.join(KAGGLE_INPUT_PATH, directory)\n",
        "    try:\n",
        "        with urlopen(download_url) as fileres, NamedTemporaryFile() as tfile:\n",
        "            total_length = fileres.headers['content-length']\n",
        "            print(f'Downloading {directory}, {total_length} bytes compressed')\n",
        "            dl = 0\n",
        "            data = fileres.read(CHUNK_SIZE)\n",
        "            while len(data) > 0:\n",
        "                dl += len(data)\n",
        "                tfile.write(data)\n",
        "                done = int(50 * dl / int(total_length))\n",
        "                sys.stdout.write(f\"\\r[{'=' * done}{' ' * (50-done)}] {dl} bytes downloaded\")\n",
        "                sys.stdout.flush()\n",
        "                data = fileres.read(CHUNK_SIZE)\n",
        "            if filename.endswith('.zip'):\n",
        "              with ZipFile(tfile) as zfile:\n",
        "                zfile.extractall(destination_path)\n",
        "            else:\n",
        "              with tarfile.open(tfile.name) as tarfile:\n",
        "                tarfile.extractall(destination_path)\n",
        "            print(f'\\nDownloaded and uncompressed: {directory}')\n",
        "    except HTTPError as e:\n",
        "        print(f'Failed to load (likely expired) {download_url} to path {destination_path}')\n",
        "        continue\n",
        "    except OSError as e:\n",
        "        print(f'Failed to load {download_url} to path {destination_path}')\n",
        "        continue\n",
        "\n",
        "print('Data source import complete.')\n"
      ],
      "metadata": {
        "id": "i7e-7X8XW2l3"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# This Python 3 environment comes with many helpful analytics libraries installed\n",
        "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
        "# For example, here's several helpful packages to load\n",
        "\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "\n",
        "# Input data files are available in the read-only \"../input/\" directory\n",
        "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
        "\n",
        "import os\n",
        "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
        "    for filename in filenames:\n",
        "        print(os.path.join(dirname, filename))\n",
        "\n",
        "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\"\n",
        "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "execution": {
          "iopub.status.busy": "2024-02-28T07:35:48.145594Z",
          "iopub.execute_input": "2024-02-28T07:35:48.14617Z",
          "iopub.status.idle": "2024-02-28T07:35:48.158618Z",
          "shell.execute_reply.started": "2024-02-28T07:35:48.146124Z",
          "shell.execute_reply": "2024-02-28T07:35:48.1572Z"
        },
        "trusted": true,
        "id": "MBx8VFHFW2l5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train dataset is read and the rows with missing target value is removed from the dataset."
      ],
      "metadata": {
        "id": "Vext3K7fW2l6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = pd.read_csv('/kaggle/input/debt-default-prediction/train.csv')\n",
        "X_train.dropna(axis=0, subset=['loan_status'], inplace=True)\n",
        "X_train.head()\n",
        "y_train = X_train.loan_status\n",
        "X_train.drop(['loan_status'], axis = 1,inplace= True )\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-02-28T07:35:48.161171Z",
          "iopub.execute_input": "2024-02-28T07:35:48.162501Z"
        },
        "trusted": true,
        "id": "SmkWSAtFW2l7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(X_train.shape)\n",
        "missing_val_count_by_column = (X_train.isnull().sum())\n",
        "print(missing_val_count_by_column[missing_val_count_by_column >0])"
      ],
      "metadata": {
        "trusted": true,
        "id": "ahaHtTHoW2l7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_cols_with_missing = (missing_val_count_by_column >50000).sum()\n",
        "num_cols_with_missing"
      ],
      "metadata": {
        "trusted": true,
        "id": "QBwABHATW2l8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Identify columns with more than 50,000 missing values\n",
        "cols_to_drop = missing_val_count_by_column[missing_val_count_by_column > 50000].index\n",
        "\n",
        "# Ensure that the columns to drop actually exist in X_train\n",
        "cols_to_drop = [col for col in cols_to_drop if col in X_train.columns]\n",
        "\n",
        "# Drop columns from X_train\n",
        "X_train.drop(cols_to_drop, axis=1, inplace=True)\n",
        "\n",
        "# Print the shape of X_train after dropping columns\n",
        "print(X_train.shape)"
      ],
      "metadata": {
        "trusted": true,
        "id": "9GRKpLMwW2l8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_valid = pd.read_csv('/kaggle/input/debt-default-prediction/valid.csv')\n",
        "X_valid.dropna(axis=0, subset=['loan_status'], inplace=True)\n",
        "\n",
        "y_valid = X_valid.loan_status\n",
        "X_valid.drop(['loan_status'], axis = 1,inplace= True )\n",
        "train_columns  = X_train.columns\n",
        "X_valid = X_valid[train_columns]\n",
        "X_valid.shape"
      ],
      "metadata": {
        "trusted": true,
        "id": "6eJbDYjmW2l8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_test = pd.read_csv('/kaggle/input/debt-default-prediction/X_test.csv')\n",
        "X_test = X_test[train_columns]\n",
        "X_test.shape"
      ],
      "metadata": {
        "trusted": true,
        "id": "w5_nv7PyW2l9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* **Let us handle the rest of the missing values with imputations , numerical values with the mean of the column and the object columns with the most frequent value.**"
      ],
      "metadata": {
        "id": "m227pb9jW2l9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "unique_values_counts = X_train.nunique()\n",
        "\n",
        "# Find columns where number of unique values is equal to 1\n",
        "columns_with_same_value = unique_values_counts[unique_values_counts == 1].index.tolist()\n",
        "\n",
        "# Print columns with the same value for every entry\n",
        "\n",
        "X_train = X_train.drop(columns=columns_with_same_value)\n",
        "X_valid = X_valid.drop(columns=columns_with_same_value)\n",
        "X_test = X_test.drop(columns=columns_with_same_value)\n",
        "print(X_train.shape, X_valid.shape, X_test.shape)"
      ],
      "metadata": {
        "trusted": true,
        "id": "JcIwj8OUW2l9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "my_imputer = SimpleImputer()\n",
        "numerical_cols = [cname for cname in X_train.columns if\n",
        "                X_train[cname].dtype in ['int64', 'float64']]\n",
        "\n",
        "categorical_cols = [cname for cname in X_train.columns if\n",
        "                    X_train[cname].dtype == 'object']\n",
        "\n",
        "numerical_imputer = SimpleImputer(strategy='mean')\n",
        "X_train_numerical = X_train[numerical_cols].copy()\n",
        "X_valid_numerical = X_valid[numerical_cols].copy()\n",
        "X_test_numerical = X_test[numerical_cols].copy()\n",
        "X_train_numerical = numerical_imputer.fit_transform(X_train_numerical)\n",
        "X_valid_numerical = numerical_imputer.transform(X_valid_numerical)\n",
        "X_test_numerical = numerical_imputer.transform(X_test_numerical)\n",
        "\n",
        "\n",
        "# Preprocessing for categorical data\n",
        "categorical_imputer = SimpleImputer(strategy='most_frequent')\n",
        "X_train_categorical = X_train[categorical_cols].copy()\n",
        "X_valid_categorical = X_valid[categorical_cols].copy()\n",
        "X_test_categorical = X_test[categorical_cols].copy()\n",
        "X_train_categorical = categorical_imputer.fit_transform(X_train_categorical)\n",
        "X_valid_categorical = categorical_imputer.transform(X_valid_categorical)\n",
        "X_test_categorical = categorical_imputer.transform(X_test_categorical)\n",
        "\n",
        "# Convert back to DataFrame\n",
        "\n",
        "X_train_numerical = pd.DataFrame(X_train_numerical, columns=numerical_cols)\n",
        "X_train_categorical = pd.DataFrame(X_train_categorical, columns=categorical_cols)\n",
        "X_valid_numerical = pd.DataFrame(X_valid_numerical, columns=numerical_cols)\n",
        "X_valid_categorical = pd.DataFrame(X_valid_categorical, columns=categorical_cols)\n",
        "X_test_numerical = pd.DataFrame(X_test_numerical, columns=numerical_cols)\n",
        "X_test_categorical = pd.DataFrame(X_test_categorical, columns=categorical_cols)\n",
        "\n",
        "\n",
        "\n",
        "# Now you can combine the numerical and categorical data\n",
        "X_train = pd.concat([X_train_numerical, X_train_categorical], axis=1)\n",
        "X_valid =  pd.concat([X_valid_numerical, X_valid_categorical], axis=1)\n",
        "X_test=  pd.concat([X_test_numerical, X_test_categorical], axis=1)\n",
        "\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "u3Ny21oQW2l-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "missing_val_count_by_column = (X_train.isnull().sum())\n",
        "print(missing_val_count_by_column[missing_val_count_by_column >0])\n",
        "missing_val_count_by_column_valid = (X_valid.isnull().sum())\n",
        "print(missing_val_count_by_column_valid[missing_val_count_by_column_valid >0])\n",
        "print(X_train.shape, X_valid.shape)"
      ],
      "metadata": {
        "trusted": true,
        "id": "Dt1JJLVAW2l-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train[categorical_cols].head()"
      ],
      "metadata": {
        "trusted": true,
        "id": "aMiYoG6fW2l-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#X_train['emp_length'].unique()"
      ],
      "metadata": {
        "trusted": true,
        "id": "ENzxdtlRW2l-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "ixhmFN6lW2l-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import OrdinalEncoder\n",
        "#custom_order_grade = ['A', 'B','C','D','E','F','G']\n",
        "custom_order_subgrade = ['A1','A2','A3','A4','A5','B1','B2','B3','B4','B5','C1','C2','C3','C4','C5','D1','D2','D3','D4','D5','E1','E2','E3','E4','E5','F1','F2','F3','F4','F5','G1','G2','G3','G4','G5']\n",
        "ordinal_encoder = OrdinalEncoder(categories=[custom_order_subgrade])\n",
        "X_train['sub_grade'] = ordinal_encoder.fit_transform(X_train[['sub_grade']])\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "1-ZEEjxfW2l_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_valid['sub_grade'] = ordinal_encoder.transform(X_valid[['sub_grade']])\n",
        "X_test['sub_grade'] = ordinal_encoder.transform(X_test[['sub_grade']])\n",
        "\n",
        "# custom_order_emp_length = ['< 1 year', '1 year', '2 years', '3 years','4 years',  '5 years','6 years', '7 years','8 years','9 years','10+ years']\n",
        "# ordinal_encoder2 = OrdinalEncoder(categories=[custom_order_emp_length])\n",
        "# X_train['emp_length'] = ordinal_encoder2.fit_transform(X_train[['emp_length']])\n",
        "# X_valid['emp_length'] = ordinal_encoder2.transform(X_valid[['emp_length']])\n",
        "# X_test['emp_length'] = ordinal_encoder2.transform(X_test[['emp_length']])\n",
        "print('done')"
      ],
      "metadata": {
        "trusted": true,
        "id": "dFRmLTqMW2l_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_valid['sub_grade'].head()"
      ],
      "metadata": {
        "trusted": true,
        "id": "XlsgkvWBW2l_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "X_train_modified= X_train.drop(columns = ['grade'])\n",
        "X_valid_modified = X_valid.drop(columns = ['grade'])\n",
        "X_test_modified = X_test.drop(columns = ['grade'])# redundant feature with grade\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "Tl-3dsM5W2l_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_valid_modified.shape"
      ],
      "metadata": {
        "trusted": true,
        "id": "GFt0usrGW2l_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# List to store features with unequal unique values\n",
        "columns_with_different_unique_values = []\n",
        "\n",
        "categorical_cols = [cname for cname in X_train_modified.columns if\n",
        "                    X_train_modified[cname].dtype == 'object']\n",
        "#print(categorical_cols)\n",
        "# Iterate over each feature\n",
        "for col in categorical_cols:\n",
        "    # Get unique values of the feature in train and validation sets\n",
        "    train_unique_values = set(X_train_modified[col].unique())\n",
        "    valid_unique_values = set(X_valid_modified[col].unique())\n",
        "    #print(col,X_train[col].nunique() ,X_valid[col].nunique())\n",
        "\n",
        "    # Check if unique values are not equal\n",
        "    if not valid_unique_values.issubset(train_unique_values):\n",
        "        columns_with_different_unique_values.append(col)\n",
        "        print(col, X_train_modified[col].nunique(),X_valid_modified[col].nunique())\n",
        "\n",
        "\n",
        "# Print features with unequal unique values\n",
        "print(\"Features with unequal unique values between X_train and X_valid:\")\n",
        "print(columns_with_different_unique_values)"
      ],
      "metadata": {
        "trusted": true,
        "id": "Pa0YWHB8W2l_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "good_label_cols = [col for col in categorical_cols if\n",
        "                   (set(X_valid_modified[col]).issubset(set(X_train_modified[col]))) and (set(X_test_modified[col]).issubset(set(X_train_modified[col])))]\n",
        "\n",
        "bad_label_cols = list(set(categorical_cols)-set(good_label_cols))\n",
        "\n",
        "print('Categorical columns that will be ordinal encoded:', good_label_cols)\n",
        "print('\\nCategorical columns that will be dropped from the dataset:', bad_label_cols)"
      ],
      "metadata": {
        "trusted": true,
        "id": "vsRZ1mAYW2l_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The bad labels are removed since in the valid data set those features contain different labels from the train dataset"
      ],
      "metadata": {
        "id": "d7eVvOPjW2l_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "pgRq5CrHW2l_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_pre_encoding = X_train_modified.drop(bad_label_cols, axis=1)\n",
        "X_valid_pre_encoding= X_valid_modified.drop(bad_label_cols, axis=1)\n",
        "X_test_pre_encoding  = X_test_modified.drop(bad_label_cols, axis=1)"
      ],
      "metadata": {
        "trusted": true,
        "id": "sOcrpmMtW2l_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "set(X_train_pre_encoding['term'].unique()) == set(X_valid_pre_encoding['term'].unique())"
      ],
      "metadata": {
        "trusted": true,
        "id": "Sn8TF-N-W2mA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_valid_pre_encoding['term'].unique()"
      ],
      "metadata": {
        "trusted": true,
        "id": "eE3wukk1W2mA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_pre_encoding['term'].unique()"
      ],
      "metadata": {
        "trusted": true,
        "id": "gRXI4400W2mA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_test_pre_encoding['term'].unique()"
      ],
      "metadata": {
        "trusted": true,
        "id": "qe5ndYvyW2mA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import OrdinalEncoder\n",
        "ordinal_encoder = OrdinalEncoder()\n",
        "X_train_encoded = X_train_pre_encoding.copy()\n",
        "X_valid_encoded = X_valid_pre_encoding.copy()\n",
        "X_test_encoded = X_test_pre_encoding.copy()\n",
        "\n",
        "print(set(X_train_encoded['term'].unique()) == set(X_valid_encoded['term'].unique()))\n",
        "X_train_encoded[good_label_cols] = ordinal_encoder.fit_transform(X_train_pre_encoding[good_label_cols])\n",
        "X_valid_encoded[good_label_cols] = ordinal_encoder.transform(X_valid_pre_encoding[good_label_cols])\n",
        "X_test_encoded[good_label_cols] = ordinal_encoder.transform(X_test_pre_encoding[good_label_cols])\n",
        "\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "j-k__IPZW2mA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "categorical_cols = [cname for cname in X_train_encoded.columns if X_train_encoded[cname].dtype == 'object']\n",
        "categorical_cols"
      ],
      "metadata": {
        "trusted": true,
        "id": "DRQGzp5iW2mA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JxU5LgejW2mA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import xgboost as xgb\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.model_selection import cross_val_score, KFold\n",
        "\n",
        "X_train_new  = X_train_encoded\n",
        "X_valid_new = X_valid_encoded\n",
        "X_test_new  = X_test_encoded\n",
        "\n",
        "# Define and train an XGBoost classifier\n",
        "model = xgb.XGBClassifier()\n",
        "model.fit(X_train_new, y_train)\n",
        "y_pred_initial = model.predict(X_valid_new)\n",
        "accuracy_initial = accuracy_score(y_valid, y_pred_initial)\n",
        "precision_initial = precision_score(y_valid, y_pred_initial)\n",
        "print(f\"Accuracy with initial features: {accuracy_initial}\")\n",
        "print(f\"precision with initial features: {precision_initial}\")\n",
        "# Get feature importance scores\n",
        "feature_importances = model.feature_importances_\n",
        "\n",
        "# Print feature importance scores\n",
        "# for i, score in enumerate(feature_importances):\n",
        "#     print(f\"Feature '{X_train_new.columns[i]}' importance: {score}\")\n",
        "\n",
        "# Use feature importance scores to select features\n",
        "selected_features = [X_train_new.columns[i] for i, score in enumerate(feature_importances) if score > 0.00005]\n",
        "\n",
        "# Select only the selected features for both training and validation sets\n",
        "X_train_selected = X_train_new[selected_features]\n",
        "X_valid_selected = X_valid_new[selected_features]\n",
        "X_test_selected = X_test_new[selected_features]\n",
        "\n",
        "# Train and evaluate the model using only selected features\n",
        "model_selected = xgb.XGBClassifier()\n",
        "model_selected.fit(X_train_selected, y_train)\n",
        "y_pred_selected = model_selected.predict(X_valid_selected)\n",
        "accuracy_selected = accuracy_score(y_valid, y_pred_selected)\n",
        "precision = precision_score(y_valid, y_pred_selected)\n",
        "print(f\"Accuracy with selected features: {accuracy_selected}\")\n",
        "print(f\"precision with selected features: {precision}\")\n",
        "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "cv_scores = cross_val_score(model_selected, X_train_selected, y_train, cv=kf, scoring='accuracy')\n",
        "\n",
        "# Print the cross-validation scores\n",
        "print(\"Cross-validation scores:\", cv_scores)\n",
        "print(\"Mean CV accuracy:\", cv_scores.mean())\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "0krd3iArW2mA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred_train = model_selected.predict(X_train_selected)\n",
        "accuracy_train = accuracy_score(y_train, y_pred_train)\n",
        "precision_train = precision_score(y_train, y_pred_train)\n",
        "print(f\"Accuracy with selected features for training data: {accuracy_train}\")\n",
        "print(f\"precision with selected features for training data: {precision_train}\")\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "OiGHvDzWW2mA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_new.shape"
      ],
      "metadata": {
        "trusted": true,
        "id": "omi0Ut1OW2mA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let us try a PCA analaysis"
      ],
      "metadata": {
        "id": "WkzD-Sj4W2mA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(X_test_new.shape)\n",
        "X_valid_new.shape"
      ],
      "metadata": {
        "trusted": true,
        "id": "ISuGHWpmW2mB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "zMupzcwqW2mB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_test_new.head()"
      ],
      "metadata": {
        "trusted": true,
        "id": "QSnbSow5W2mE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_valid_new.head()"
      ],
      "metadata": {
        "trusted": true,
        "id": "7mqZwPwPW2mE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import PCA\n",
        "pca = PCA(n_components = 20)\n",
        "X_train_pca = pca.fit_transform(X_train_selected)\n",
        "component_names  = [f\"PC{i+1}\" for i in range(X_train_pca.shape[1])]\n",
        "X_train_pca = pd.DataFrame(X_train_pca, columns= component_names)\n",
        "X_valid_pca = pca.transform(X_valid_selected)\n",
        "X_valid_pca = pd.DataFrame(X_valid_pca, columns= component_names)\n",
        "X_valid_pca.head()\n",
        "#X_pca.head()"
      ],
      "metadata": {
        "trusted": true,
        "id": "ihtc3-_7W2mE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loadings   = pd.DataFrame(pca.components_.T,\n",
        "                         columns  = component_names,\n",
        "\n",
        "                         )\n",
        "loadings = pd.DataFrame(pca.components_.T, columns=component_names)\n",
        "loadings"
      ],
      "metadata": {
        "trusted": true,
        "id": "aITvOW2XW2mE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_with_pca = xgb.XGBClassifier()\n",
        "model_with_pca.fit(X_train_pca, y_train)\n",
        "y_pred_pca = model_with_pca.predict(X_valid_pca)\n",
        "accuracy_pca = accuracy_score(y_valid, y_pred_pca)\n",
        "precision_pca = precision_score(y_valid, y_pred_pca)\n",
        "print(f\"Accuracy with selected features: {accuracy_pca}\")\n",
        "print(f\"precision with selected features: {precision_pca}\")"
      ],
      "metadata": {
        "trusted": true,
        "id": "YwHXLR9UW2mE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_test = model_selected.predict(X_test_selected)\n",
        "\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "9vwXrC32W2mE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_test_df = pd.DataFrame(y_test, columns=['predicted_label'])\n",
        "combined_df = pd.concat([y_test_df, X_test_selected], axis=1)\n",
        "\n",
        "# Save the combined DataFrame to a CSV file\n",
        "combined_df.to_csv('210670N.csv', index=False)"
      ],
      "metadata": {
        "trusted": true,
        "id": "mLZyL9TeW2mE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_test_df.head()"
      ],
      "metadata": {
        "trusted": true,
        "id": "3YvUd4TFW2mE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "combined_df.to_csv('/kaggle/working/210670N.csv', index=False)"
      ],
      "metadata": {
        "trusted": true,
        "id": "wpLYFhcjW2mE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import FileLink\n",
        "FileLink(r'210670N.csv')"
      ],
      "metadata": {
        "trusted": true,
        "id": "PR6PBs5fW2mF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_selected.shape"
      ],
      "metadata": {
        "trusted": true,
        "id": "hV4up3UzW2mF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JJX5YEocW2mF"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}