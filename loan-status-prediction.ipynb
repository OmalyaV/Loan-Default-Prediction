{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":7653020,"sourceType":"datasetVersion","datasetId":4461571}],"dockerImageVersionId":30646,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-02-26T18:09:20.159199Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Train dataset is read and the rows with missing target value is removed from the dataset.","metadata":{}},{"cell_type":"code","source":"X_train = pd.read_csv('/kaggle/input/debt-default-prediction/train.csv')\nX_train.dropna(axis=0, subset=['loan_status'], inplace=True)\nX_train.head()\ny_train = X_train.loan_status\nX_train.drop(['loan_status'], axis = 1,inplace= True )\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(X_train.shape)\nmissing_val_count_by_column = (X_train.isnull().sum())\nprint(missing_val_count_by_column[missing_val_count_by_column >0])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"num_cols_with_missing = (missing_val_count_by_column >100000).sum()\nnum_cols_with_missing","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Identify columns with more than 50,000 missing values\ncols_to_drop = missing_val_count_by_column[missing_val_count_by_column > 100000].index\n\n# Ensure that the columns to drop actually exist in X_train\ncols_to_drop = [col for col in cols_to_drop if col in X_train.columns]\n\n# Drop columns from X_train\nX_train.drop(cols_to_drop, axis=1, inplace=True)\n\n# Print the shape of X_train after dropping columns\nprint(X_train.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_valid = pd.read_csv('/kaggle/input/debt-default-prediction/valid.csv')\nX_valid.dropna(axis=0, subset=['loan_status'], inplace=True)\n\ny_valid = X_valid.loan_status\nX_valid.drop(['loan_status'], axis = 1,inplace= True )\ntrain_columns  = X_train.columns\nX_valid = X_valid[train_columns]\nX_valid.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* **Let us handle the rest of the missing values with imputations , numerical values with the mean of the column and the object columns with the most frequent value.**","metadata":{}},{"cell_type":"code","source":"unique_values_counts = X_train.nunique()\n\n# Find columns where number of unique values is equal to 1\ncolumns_with_same_value = unique_values_counts[unique_values_counts == 1].index.tolist()\n\n# Print columns with the same value for every entry\n\nX_train = X_train.drop(columns=columns_with_same_value)\nX_valid = X_valid.drop(columns=columns_with_same_value)\nprint(X_train.shape, X_valid.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\n\nmy_imputer = SimpleImputer()\nnumerical_cols = [cname for cname in X_train.columns if \n                X_train[cname].dtype in ['int64', 'float64']]\n\ncategorical_cols = [cname for cname in X_train.columns if \n                    X_train[cname].dtype == 'object']\n\nnumerical_imputer = SimpleImputer(strategy='mean')\nX_train_numerical = X_train[numerical_cols].copy()\nX_valid_numerical = X_valid[numerical_cols].copy()\nX_train_numerical = numerical_imputer.fit_transform(X_train_numerical)\nX_valid_numerical = numerical_imputer.transform(X_valid_numerical)\n\n# Preprocessing for categorical data\ncategorical_imputer = SimpleImputer(strategy='most_frequent')\nX_train_categorical = X_train[categorical_cols].copy()\nX_valid_categorical = X_valid[categorical_cols].copy()\nX_train_categorical = categorical_imputer.fit_transform(X_train_categorical)\nX_valid_categorical = categorical_imputer.transform(X_valid_categorical)\n\n# Convert back to DataFrame\n\nX_train_numerical = pd.DataFrame(X_train_numerical, columns=numerical_cols)\nX_train_categorical = pd.DataFrame(X_train_categorical, columns=categorical_cols)\nX_valid_numerical = pd.DataFrame(X_valid_numerical, columns=numerical_cols)\nX_valid_categorical = pd.DataFrame(X_valid_categorical, columns=categorical_cols)\n\n\n# Now you can combine the numerical and categorical data\nX_train = pd.concat([X_train_numerical, X_train_categorical], axis=1)\nX_valid =  pd.concat([X_valid_numerical, X_valid_categorical], axis=1)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"missing_val_count_by_column = (X_train.isnull().sum())\nprint(missing_val_count_by_column[missing_val_count_by_column >0])\nmissing_val_count_by_column_valid = (X_valid.isnull().sum())\nprint(missing_val_count_by_column_valid[missing_val_count_by_column_valid >0])\nprint(X_train.shape, X_valid.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train[categorical_cols].head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train['emp_length'].unique()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import OrdinalEncoder\n#custom_order_grade = ['A', 'B','C','D','E','F','G']\ncustom_order_subgrade = ['A1','A2','A3','A4','A5','B1','B2','B3','B4','B5','C1','C2','C3','C4','C5','D1','D2','D3','D4','D5','E1','E2','E3','E4','E5','F1','F2','F3','F4','F5','G1','G2','G3','G4','G5']\nordinal_encoder = OrdinalEncoder(categories=[custom_order_subgrade])\nX_train['sub_grade'] = ordinal_encoder.fit_transform(X_train[['sub_grade']])\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_valid['sub_grade'] = ordinal_encoder.transform(X_valid[['sub_grade']]) \ncustom_order_emp_length = ['< 1 year', '1 year', '2 years', '3 years','4 years',  '5 years','6 years', '7 years','8 years','9 years','10+ years']\nordinal_encoder2 = OrdinalEncoder(categories=[custom_order_emp_length])\nX_train['emp_length'] = ordinal_encoder2.fit_transform(X_train[['emp_length']])\nX_valid['emp_length'] = ordinal_encoder2.transform(X_valid[['emp_length']])\nprint('done')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_valid['sub_grade'].head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nX_train_modified= X_train.drop(columns = ['grade'])\nX_valid_modified = X_valid.drop(columns = ['grade']) # redundant feature with grade\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_valid_modified.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# List to store features with unequal unique values\ncolumns_with_different_unique_values = []\n\ncategorical_cols = [cname for cname in X_train_modified.columns if \n                    X_train_modified[cname].dtype == 'object']\n#print(categorical_cols)\n# Iterate over each feature\nfor col in categorical_cols:\n    # Get unique values of the feature in train and validation sets\n    train_unique_values = set(X_train_modified[col].unique())\n    valid_unique_values = set(X_valid_modified[col].unique())\n    #print(col,X_train[col].nunique() ,X_valid[col].nunique())\n    \n    # Check if unique values are not equal\n    if not valid_unique_values.issubset(train_unique_values):\n        columns_with_different_unique_values.append(col)\n        print(col, X_train_modified[col].nunique(),X_valid_modified[col].nunique())\n\n\n# Print features with unequal unique values\nprint(\"Features with unequal unique values between X_train and X_valid:\")\nprint(columns_with_different_unique_values)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"good_label_cols = [col for col in categorical_cols if \n                   set(X_valid_modified[col]).issubset(set(X_train_modified[col]))]\n\nbad_label_cols = list(set(categorical_cols)-set(good_label_cols))\n        \nprint('Categorical columns that will be ordinal encoded:', good_label_cols)\nprint('\\nCategorical columns that will be dropped from the dataset:', bad_label_cols)        ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The bad labels are removed since in the valid data set those features contain different labels from the train dataset","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"X_train_pre_encoding = X_train_modified.drop(bad_label_cols, axis=1)\nX_valid_pre_encoding= X_valid_modified.drop(bad_label_cols, axis=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train_pre_encoding.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\nlabel_encoder = LabelEncoder()\nX_train_encoded = X_train_pre_encoding.copy()\nX_valid_encoded = X_valid_pre_encoding.copy()\nfor column in good_label_cols:  # Iterate over categorical columns\n    X_train_encoded[column] = label_encoder.fit_transform(X_train_pre_encoding[column])\nfor column in good_label_cols:  # Iterate over categorical columns\n    X_valid_encoded[column] = label_encoder.transform(X_valid_pre_encoding[column])\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}