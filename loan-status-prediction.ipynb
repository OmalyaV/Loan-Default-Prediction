{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":7653020,"sourceType":"datasetVersion","datasetId":4461571}],"dockerImageVersionId":30646,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-02-28T07:35:48.145594Z","iopub.execute_input":"2024-02-28T07:35:48.146170Z","iopub.status.idle":"2024-02-28T07:35:48.158618Z","shell.execute_reply.started":"2024-02-28T07:35:48.146124Z","shell.execute_reply":"2024-02-28T07:35:48.157200Z"},"trusted":true},"execution_count":131,"outputs":[{"name":"stdout","text":"/kaggle/input/debt-default-prediction/X_test.csv\n/kaggle/input/debt-default-prediction/DataDictionary.xlsx\n/kaggle/input/debt-default-prediction/valid.csv\n/kaggle/input/debt-default-prediction/train.csv\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Train dataset is read and the rows with missing target value is removed from the dataset.","metadata":{}},{"cell_type":"code","source":"X_train = pd.read_csv('/kaggle/input/debt-default-prediction/train.csv')\nX_train.dropna(axis=0, subset=['loan_status'], inplace=True)\nX_train.head()\ny_train = X_train.loan_status\nX_train.drop(['loan_status'], axis = 1,inplace= True )\n","metadata":{"execution":{"iopub.status.busy":"2024-02-28T07:35:48.161171Z","iopub.execute_input":"2024-02-28T07:35:48.162501Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(X_train.shape)\nmissing_val_count_by_column = (X_train.isnull().sum())\nprint(missing_val_count_by_column[missing_val_count_by_column >0])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"num_cols_with_missing = (missing_val_count_by_column >50000).sum()\nnum_cols_with_missing","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Identify columns with more than 50,000 missing values\ncols_to_drop = missing_val_count_by_column[missing_val_count_by_column > 50000].index\n\n# Ensure that the columns to drop actually exist in X_train\ncols_to_drop = [col for col in cols_to_drop if col in X_train.columns]\n\n# Drop columns from X_train\nX_train.drop(cols_to_drop, axis=1, inplace=True)\n\n# Print the shape of X_train after dropping columns\nprint(X_train.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_valid = pd.read_csv('/kaggle/input/debt-default-prediction/valid.csv')\nX_valid.dropna(axis=0, subset=['loan_status'], inplace=True)\n\ny_valid = X_valid.loan_status\nX_valid.drop(['loan_status'], axis = 1,inplace= True )\ntrain_columns  = X_train.columns\nX_valid = X_valid[train_columns]\nX_valid.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_test = pd.read_csv('/kaggle/input/debt-default-prediction/X_test.csv')\nX_test = X_test[train_columns]\nX_test.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* **Let us handle the rest of the missing values with imputations , numerical values with the mean of the column and the object columns with the most frequent value.**","metadata":{}},{"cell_type":"code","source":"unique_values_counts = X_train.nunique()\n\n# Find columns where number of unique values is equal to 1\ncolumns_with_same_value = unique_values_counts[unique_values_counts == 1].index.tolist()\n\n# Print columns with the same value for every entry\n\nX_train = X_train.drop(columns=columns_with_same_value)\nX_valid = X_valid.drop(columns=columns_with_same_value)\nX_test = X_test.drop(columns=columns_with_same_value)\nprint(X_train.shape, X_valid.shape, X_test.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\n\nmy_imputer = SimpleImputer()\nnumerical_cols = [cname for cname in X_train.columns if \n                X_train[cname].dtype in ['int64', 'float64']]\n\ncategorical_cols = [cname for cname in X_train.columns if \n                    X_train[cname].dtype == 'object']\n\nnumerical_imputer = SimpleImputer(strategy='mean')\nX_train_numerical = X_train[numerical_cols].copy()\nX_valid_numerical = X_valid[numerical_cols].copy()\nX_test_numerical = X_test[numerical_cols].copy()\nX_train_numerical = numerical_imputer.fit_transform(X_train_numerical)\nX_valid_numerical = numerical_imputer.transform(X_valid_numerical)\nX_test_numerical = numerical_imputer.transform(X_test_numerical)\n\n\n# Preprocessing for categorical data\ncategorical_imputer = SimpleImputer(strategy='most_frequent')\nX_train_categorical = X_train[categorical_cols].copy()\nX_valid_categorical = X_valid[categorical_cols].copy()\nX_test_categorical = X_test[categorical_cols].copy()\nX_train_categorical = categorical_imputer.fit_transform(X_train_categorical)\nX_valid_categorical = categorical_imputer.transform(X_valid_categorical)\nX_test_categorical = categorical_imputer.transform(X_test_categorical)\n\n# Convert back to DataFrame\n\nX_train_numerical = pd.DataFrame(X_train_numerical, columns=numerical_cols)\nX_train_categorical = pd.DataFrame(X_train_categorical, columns=categorical_cols)\nX_valid_numerical = pd.DataFrame(X_valid_numerical, columns=numerical_cols)\nX_valid_categorical = pd.DataFrame(X_valid_categorical, columns=categorical_cols)\nX_test_numerical = pd.DataFrame(X_test_numerical, columns=numerical_cols)\nX_test_categorical = pd.DataFrame(X_test_categorical, columns=categorical_cols)\n\n\n\n# Now you can combine the numerical and categorical data\nX_train = pd.concat([X_train_numerical, X_train_categorical], axis=1)\nX_valid =  pd.concat([X_valid_numerical, X_valid_categorical], axis=1)\nX_test=  pd.concat([X_test_numerical, X_test_categorical], axis=1)\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"missing_val_count_by_column = (X_train.isnull().sum())\nprint(missing_val_count_by_column[missing_val_count_by_column >0])\nmissing_val_count_by_column_valid = (X_valid.isnull().sum())\nprint(missing_val_count_by_column_valid[missing_val_count_by_column_valid >0])\nprint(X_train.shape, X_valid.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train[categorical_cols].head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#X_train['emp_length'].unique()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import OrdinalEncoder\n#custom_order_grade = ['A', 'B','C','D','E','F','G']\ncustom_order_subgrade = ['A1','A2','A3','A4','A5','B1','B2','B3','B4','B5','C1','C2','C3','C4','C5','D1','D2','D3','D4','D5','E1','E2','E3','E4','E5','F1','F2','F3','F4','F5','G1','G2','G3','G4','G5']\nordinal_encoder = OrdinalEncoder(categories=[custom_order_subgrade])\nX_train['sub_grade'] = ordinal_encoder.fit_transform(X_train[['sub_grade']])\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_valid['sub_grade'] = ordinal_encoder.transform(X_valid[['sub_grade']]) \nX_test['sub_grade'] = ordinal_encoder.transform(X_test[['sub_grade']]) \n\n# custom_order_emp_length = ['< 1 year', '1 year', '2 years', '3 years','4 years',  '5 years','6 years', '7 years','8 years','9 years','10+ years']\n# ordinal_encoder2 = OrdinalEncoder(categories=[custom_order_emp_length])\n# X_train['emp_length'] = ordinal_encoder2.fit_transform(X_train[['emp_length']])\n# X_valid['emp_length'] = ordinal_encoder2.transform(X_valid[['emp_length']])\n# X_test['emp_length'] = ordinal_encoder2.transform(X_test[['emp_length']])\nprint('done')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_valid['sub_grade'].head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nX_train_modified= X_train.drop(columns = ['grade'])\nX_valid_modified = X_valid.drop(columns = ['grade'])\nX_test_modified = X_test.drop(columns = ['grade'])# redundant feature with grade\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_valid_modified.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# List to store features with unequal unique values\ncolumns_with_different_unique_values = []\n\ncategorical_cols = [cname for cname in X_train_modified.columns if \n                    X_train_modified[cname].dtype == 'object']\n#print(categorical_cols)\n# Iterate over each feature\nfor col in categorical_cols:\n    # Get unique values of the feature in train and validation sets\n    train_unique_values = set(X_train_modified[col].unique())\n    valid_unique_values = set(X_valid_modified[col].unique())\n    #print(col,X_train[col].nunique() ,X_valid[col].nunique())\n    \n    # Check if unique values are not equal\n    if not valid_unique_values.issubset(train_unique_values):\n        columns_with_different_unique_values.append(col)\n        print(col, X_train_modified[col].nunique(),X_valid_modified[col].nunique())\n\n\n# Print features with unequal unique values\nprint(\"Features with unequal unique values between X_train and X_valid:\")\nprint(columns_with_different_unique_values)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"good_label_cols = [col for col in categorical_cols if \n                   (set(X_valid_modified[col]).issubset(set(X_train_modified[col]))) and (set(X_test_modified[col]).issubset(set(X_train_modified[col])))]\n\nbad_label_cols = list(set(categorical_cols)-set(good_label_cols))\n        \nprint('Categorical columns that will be ordinal encoded:', good_label_cols)\nprint('\\nCategorical columns that will be dropped from the dataset:', bad_label_cols)        ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The bad labels are removed since in the valid data set those features contain different labels from the train dataset","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"X_train_pre_encoding = X_train_modified.drop(bad_label_cols, axis=1)\nX_valid_pre_encoding= X_valid_modified.drop(bad_label_cols, axis=1)\nX_test_pre_encoding  = X_test_modified.drop(bad_label_cols, axis=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"set(X_train_pre_encoding['term'].unique()) == set(X_valid_pre_encoding['term'].unique())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_valid_pre_encoding['term'].unique()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train_pre_encoding['term'].unique()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_test_pre_encoding['term'].unique()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.preprocessing import OrdinalEncoder\nordinal_encoder = OrdinalEncoder()\nX_train_encoded = X_train_pre_encoding.copy()\nX_valid_encoded = X_valid_pre_encoding.copy()\nX_test_encoded = X_test_pre_encoding.copy()\n\nprint(set(X_train_encoded['term'].unique()) == set(X_valid_encoded['term'].unique()))\nX_train_encoded[good_label_cols] = ordinal_encoder.fit_transform(X_train_pre_encoding[good_label_cols])\nX_valid_encoded[good_label_cols] = ordinal_encoder.transform(X_valid_pre_encoding[good_label_cols])\nX_test_encoded[good_label_cols] = ordinal_encoder.transform(X_test_pre_encoding[good_label_cols])\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"categorical_cols = [cname for cname in X_train_encoded.columns if X_train_encoded[cname].dtype == 'object']\ncategorical_cols","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import xgboost as xgb\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import precision_score\nfrom sklearn.model_selection import cross_val_score, KFold\n\nX_train_new  = X_train_encoded\nX_valid_new = X_valid_encoded\nX_test_new  = X_test_encoded\n\n# Define and train an XGBoost classifier\nmodel = xgb.XGBClassifier()\nmodel.fit(X_train_new, y_train)\ny_pred_initial = model.predict(X_valid_new)\naccuracy_initial = accuracy_score(y_valid, y_pred_initial)\nprecision_initial = precision_score(y_valid, y_pred_initial)\nprint(f\"Accuracy with initial features: {accuracy_initial}\")\nprint(f\"precision with initial features: {precision_initial}\")\n# Get feature importance scores\nfeature_importances = model.feature_importances_\n\n# Print feature importance scores\n# for i, score in enumerate(feature_importances):\n#     print(f\"Feature '{X_train_new.columns[i]}' importance: {score}\")\n\n# Use feature importance scores to select features\nselected_features = [X_train_new.columns[i] for i, score in enumerate(feature_importances) if score > 0.00005]\n\n# Select only the selected features for both training and validation sets\nX_train_selected = X_train_new[selected_features]\nX_valid_selected = X_valid_new[selected_features]\nX_test_selected = X_test_new[selected_features]\n\n# Train and evaluate the model using only selected features\nmodel_selected = xgb.XGBClassifier()\nmodel_selected.fit(X_train_selected, y_train)\ny_pred_selected = model_selected.predict(X_valid_selected)\naccuracy_selected = accuracy_score(y_valid, y_pred_selected)\nprecision = precision_score(y_valid, y_pred_selected)\nprint(f\"Accuracy with selected features: {accuracy_selected}\")\nprint(f\"precision with selected features: {precision}\")\nkf = KFold(n_splits=5, shuffle=True, random_state=42)\ncv_scores = cross_val_score(model_selected, X_train_selected, y_train, cv=kf, scoring='accuracy')\n\n# Print the cross-validation scores\nprint(\"Cross-validation scores:\", cv_scores)\nprint(\"Mean CV accuracy:\", cv_scores.mean())\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred_train = model_selected.predict(X_train_selected)\naccuracy_train = accuracy_score(y_train, y_pred_train)\nprecision_train = precision_score(y_train, y_pred_train)\nprint(f\"Accuracy with selected features for training data: {accuracy_train}\")\nprint(f\"precision with selected features for training data: {precision_train}\")\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train_new.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now let us try a PCA analaysis","metadata":{}},{"cell_type":"code","source":"print(X_test_new.shape)\nX_valid_new.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"X_test_new.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_valid_new.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.decomposition import PCA\npca = PCA(n_components = 20)\nX_train_pca = pca.fit_transform(X_train_selected)\ncomponent_names  = [f\"PC{i+1}\" for i in range(X_train_pca.shape[1])]\nX_train_pca = pd.DataFrame(X_train_pca, columns= component_names)\nX_valid_pca = pca.transform(X_valid_selected)\nX_valid_pca = pd.DataFrame(X_valid_pca, columns= component_names)\nX_valid_pca.head() \n#X_pca.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"loadings   = pd.DataFrame(pca.components_.T,\n                         columns  = component_names,\n                         \n                         )\nloadings = pd.DataFrame(pca.components_.T, columns=component_names)\nloadings","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_with_pca = xgb.XGBClassifier()\nmodel_with_pca.fit(X_train_pca, y_train)\ny_pred_pca = model_with_pca.predict(X_valid_pca)\naccuracy_pca = accuracy_score(y_valid, y_pred_pca)\nprecision_pca = precision_score(y_valid, y_pred_pca)\nprint(f\"Accuracy with selected features: {accuracy_pca}\")\nprint(f\"precision with selected features: {precision_pca}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_test = model_selected.predict(X_test_selected)\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_test_df = pd.DataFrame(y_test, columns=['predicted_label'])\ncombined_df = pd.concat([y_test_df, X_test_selected], axis=1)\n\n# Save the combined DataFrame to a CSV file\ncombined_df.to_csv('210670N.csv', index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_test_df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"combined_df.to_csv('/kaggle/working/210670N.csv', index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from IPython.display import FileLink\nFileLink(r'210670N.csv')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train_selected.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}